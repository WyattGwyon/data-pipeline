# data-pipeline

Here we are looking at the buying habits at supermarkets, pulling from a csv file of a year's worth of purchases from 2014-2015. Unfortunately, we don't know the precise location of the purchases but we will deduce that it is an English speaking country. A closer inspection at itemDescription shows us that certian names of products might have roots in UK English. For, example UHT milk is a ulta-high temperture pasteurization that is very common in Europe but not in the US, and brown bread which is more commonly called whole wheat bread in the US. Thus, we are going to say for the moment that this is a list of groceries bought somewhere in the UK. 

Now, we can do a few processes to enrich this list. One obvious approach would be to fill out the dataset with purchase data from the current year of a UK chain-store such as Tescos or Morrisons. Another approach would be to collect prices of items and findout the amount spent on each category of items. A more interesting idea is to find historical weather data in the UK (possibly a high population area such as London) and try to draw some correlation between the weather and types of items purchased at the supermarket. We can make some obvious connections like chicken soup for a cold rainy day or ice cream in the summer months. The general direction to take this data is Market Basket Analysis. This is a strategy that supermarkets stand to benefit from. If they can accurately predict the probability of one item purchase in relation to another item purchase, they can use this information to put these items together on special offer or in proximity on their shelves or website. Measuring the frequency of certai purchases could be good for helful reminders on a website that the weekly supply of milk, bread, or toilet paper is likely running low. 

Ideally, Tescos would have been better but they have suspended their API for the moment. Morrisons was far too complicated for the current skill level and time investment. The correleation with weather was sidelined due to difficulty accessing historical weather information. For the project at hand, we had to defer to an imprecise collection of prices from a random super market website that provided a relatively easy opportunity for web scraping some of their data. We found Waitrose's clean presentation and reduced brand-name count easier to manage as they show support for their in-house brand a great deal more, which simplified the process of finding a yogurt purchase, for example.

The results are unfinished. Unfortunately, the technical difficulties of web scraping and cleaning data still need to be resolved but it is a good start and a interesting experiment for improving our skills.
